#home 

#Inbox 
## TODO_trivial

- [Qwen2.5-Math-Evaluation](https://github.com/QwenLM/Qwen2.5-Math/tree/main/evaluation)
- [DeepSeekæ¦œå•å¤åˆ»](https://www.deepseek.com/)
- é€‚åˆo1è¯„æµ‹çš„æ¦œå•
- battleé‡æ„
- ğŸ”¥ chaté¡µé¢è‡ªå·±å¼€å‘è¿˜æ˜¯ä½¿ç”¨å¼€æºï¼Ÿ
	- lobechatè°ƒç ”
- å¹³å°ä¼˜åŒ–
	- ä»»åŠ¡åˆ†å‘
	- æ¡†æ¶ä¼˜åŒ–
	- æ¶æ„å‡çº§
- model & chat å‡çº§ inferä¼˜åŒ–
	- å¿«é€Ÿä¸Šçº¿
	- å¿«é€Ÿå¯¹è¯
	- local & apiç»Ÿä¸€ç®¡ç†
- llama factoryäº†è§£
- æ¢ç´¢æ–°çš„è½¯ä»¶å¼€å‘æ–¹å¼å’Œæµç¨‹
	- æ¡†æ¶å»ºç«‹
	- éœ€æ±‚æ¢³ç†
	- æµ‹è¯•ç¼–å†™
	- ä»£ç ç”Ÿæˆ
## TODO_zen

- çœ‹ä¸‹llamafactoryæºç 
	- ç›®æ ‡ï¼š1. äº†è§£ä¸‹æˆ‘ä»¬çš„aiä¸­å°æ˜¯å¦‚ä½•ä½¿ç”¨llama factoryçš„ 2. äº†è§£llama factoryæ˜¯å¦‚ä½•å¾®è°ƒæ¨¡å‹çš„
- çœ‹ä¸‹mooncakeæºç 
	- æµ‹è¯•ä¸‹èƒ½å¦ä½¿ç”¨p2p transferæ¥åŠ è½½æ¨¡å‹
- äº†è§£[# ZhiLightå¤§æ¨¡å‹æ¨ç†å¼•æ“](https://github.com/zhihu/ZhiLight)
- äº†è§£DeepSeekæŠ€æœ¯æŠ¥å‘Š
- [nemo framework](https://www.nvidia.cn/ai-data-science/generative-ai/nemo-framework/)
- ğŸ”¥ å‡ºè¯¾ï¼šæƒ³ç³»ç»Ÿåœ°å­¦ä¹ æ€ä¹ˆåšä¸ªå‰ç«¯AIå¤§æ¨¡å‹åº”ç”¨ï¼Œæ–¹ä¾¿æ‰¾å·¥ä½œï¼Œæœ‰æ²¡æœ‰å¤§ä½¬æ¨èä¸€ä¸‹å¥½çš„è¯¾ç¨‹
- æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼štrlï¼Œopenrlhfï¼Œnemo aligner
## TODO_backlog

- è®¤è¯ï¼šhttps://www.hiascend.com/edu/certification/detail/34bf904cb410497cb9c582be6c047ff7
- çŸ¥è¯†å¬å› embedding duckdb åŠ asking
	- åšä¸€ä¸ªç®€å•çš„åŸºäºæŸä¸ªç½‘ç«™çš„çŸ¥è¯†æœç´¢ï¼Œlike ray?
- https://ai-bot.cn/tulu-3-ai2/ å‚è€ƒå…¶è¯„æµ‹
- [Qwen2.5æ¦œå•](https://qwen2.org/qwen2-5/)
- å…³äºæ ¡å›­éœ¸å‡ŒæŠ–éŸ³æœä¸‹
- Agentæ€»ç»“ï¼š
	- https://www.anthropic.com/research/building-effective-agents
	- `https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461149299&idx=1&sn=cee214193dcced281e1d623475458bf4&chksm=86522f0fe59b4f0090ce71da75a55c53b5c3e53b4b2b6b840ad8fcdd35f0075331502a0b6e48&scene=0&xtrack=1&version=4.1.30.99529&platform=mac&nwr_flag=1#wechat_redirect]`
- obsidian æ’ä»¶ tree mind
- https://link.zhihu.com/?target=http%3A//incompleteideas.net/book/the-book-2nd.html
- 
# åˆ†ç±»
## AI


## ç›®æ ‡

- Agent Stack
	![[Pasted image 20241126110340.png|400]]

#Inbox 

## Recent Docs

[[241111_Git PreCommit for Python]]
[[241114_How o1 worksè®ºæ–‡]]


## REINFORCE

REINFORCE++: A Simple and Efficient

Approach for Aligning Large Language Models

REINFORCE++ï¼šä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•[]()

Abstract æ‘˜è¦

Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical approach for aligning large language models with human preferences, witnessing rapid algorithmic evolution through methods such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We present REINFORCE++, an enhanced variant of the classical REINFORCE algorithm that incorporates key optimization techniques from PPO while eliminating the need for a critic network. REINFORCE + + achieves three primary objectives: (1) simplicity (2) enhanced training stability, and (3) reduced computational overhead. Through extensive empirical evaluation, we demonstrate that REINFORCE + + exhibits superior stability compared to GRPO and achieves greater computational efficiency than PPO while maintaining comparable performance. The implementation is available at https: github. com/OpenRLHF/OpenRLHF.

äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦æ–¹æ³•ï¼Œè§è¯äº†é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ã€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€REINFORCE ç•™ä¸€æ³•ï¼ˆRLOOï¼‰ã€ReMax å’Œå›¢ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰æ–¹æ³•çš„å¿«é€Ÿç®—æ³•æ¼”å˜ã€‚æˆ‘ä»¬æå‡ºäº† REINFORCE++ï¼Œè¿™æ˜¯ç»å…¸ REINFORCE ç®—æ³•çš„å¢å¼ºå˜ä½“ï¼Œå®ƒç»“åˆäº† PPO çš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¶ˆé™¤äº†å¯¹è¯„è®ºç½‘ç»œçš„éœ€æ±‚ã€‚REINFORCE++å®ç°äº†ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šï¼ˆ1ï¼‰ç®€å•æ€§ï¼ˆ2ï¼‰å¢å¼ºçš„è®­ç»ƒç¨³å®šæ€§å’Œï¼ˆ3ï¼‰å‡å°‘è®¡ç®—å¼€é”€ã€‚é€šè¿‡å¹¿æ³›çš„ç»éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº† REINFORCE++åœ¨ç¨³å®šæ€§ä¸Šä¼˜äºGRPOï¼Œå¹¶ä¸”åœ¨ä¿æŒå¯æ¯”æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ¯” PPO æ›´å¤§çš„è®¡ç®—æ•ˆç‡ã€‚è¯¥å®ç°å¯åœ¨ https://github.com/OpenRLHF/OpenRLHF è·å–ã€‚