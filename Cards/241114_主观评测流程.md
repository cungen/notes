#Inbox #Area/AI/Eval 

# 介绍

[OpenCompass主观评测指引](https://opencompass.readthedocs.io/zh-cn/latest/advanced_guides/subjective_evaluation.html)

主要类型
- Score模式，mode = singlescore，根据checklist打分并给出依据
- Compare模式，mode = m2n，base_models(m) to compare_model(n)，对比2个模型好坏，并计算胜率

# 流程

## OpenCompass运行流程

> OpenCompass评测步骤见[[20241107_OpenCompass执行问题及流程梳理]]

## 流程对比

客观评测流程：
`infer` --> `eval (score)` --> `summary`

主观评测流程
`infer` --> `eval (LMEvaluator)` --> `summary(score)`

# 配置

- models同客观
- datasets
	- singlescore时，同主观
	- m2n时，需要配置base_models
- eval改动，无法在数据集配置中修改
	```python
eval = dict(
    partitioner=dict(
        type=SubjectiveNaivePartitioner, # 任务分组修改
        models=models,
        judge_models=judge_models,
    ),
    runner=dict(
        # the api server may rate limited, so we only use one worker
        type=LocalRunner, max_num_workers=1, task=dict(type=SubjectiveEvalTask) # runner修改
    ),
)
```
- summary改动，无法在数据集中修改
	```python
summarizer = dict(type=SubjectiveSummarizer, function="subjective")
```

> eval和summary的改动，导致了无法仅通过添加数据集配置和数据集class来完成，需要生成单独的`eval_xxx.py`配置

# 执行细节

1. `packages/holmes_ray/actors/opencompass.py` , line 188,
	非_gen,  ppl结尾以及非特定数据集的，都算为主观评测
2. 主观评测执行脚本
	```bash
judge_model_name=xx judge_openai_api_base=xx judge_openai_api_key=xx python tools/hh_subjective_config_generator.py --datasets xx --max-out-len xx -w xx
```
3. `tools/hh_subjective_config_generator.py` 
	1. 补充env配置，生成`eval_xx.py`
	2. `subprocess.Popen` 执行 `python run eval_xx.py`
4. 中间结果
	1. predictions
	2. results -> `compare_model`\_judged_by--judge_model/\*\*.json
	3. 
5. Summary逻辑