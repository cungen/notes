#Area/AI/RL 

时序差分方法（Temporal Difference Learning, TD Learning）是一种结合了蒙特卡罗方法和动态规划思想的强化学习算法。它通过逐步更新值函数来学习策略，无需等待一个完整的回合结束，适用于在线学习和非稳态环境。

> **简单理解**：在已知平均分的情况下，如果增加了一个样本（或抽样），计算平均分的方式即为时序差分方法

### 核心思想
时序差分方法的核心是利用当前估计和后续估计的差异来更新值函数。

* **TD误差**：TD误差是当前奖励与后续状态值函数估计的差异（*可以理解为新样本与均值的误差*），公式为：$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$$
* **更新值函数**：使用TD误差更新当前状态的值函数（*如果$\alpha$为总数量，则可理解为更新均值*）：$$V(S_t) \leftarrow V(S_t) + \alpha \delta_t$$其中，\( \alpha \) 是学习率，控制更新幅度。

### 优点
- **在线学习**：无需等待回合结束，可以实时更新值函数。
- **高效性**：相比蒙特卡罗方法，TD方法通常收敛更快。
- **灵活性**：适用于非稳态环境和在线任务