#Area/AI/Eval 
## 介绍

llama官网榜单：[https://www.llama.com/](https://www.llama.com/)

![[Pasted image 20241212181556.png|600]]

Model Card里也有成绩相关的描述

[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)

具体的评测数据见：[https://huggingface.co/datasets/meta-llama/Llama-3.3-70B-Instruct-evals](https://huggingface.co/datasets/meta-llama/Llama-3.3-70B-Instruct-evals)

包含了12个tasks：human_eval, mmlu_pro, gpqa_diamond, ifeval__loose, mmlu__0_shot__cot, nih__multi_needle, mgsm, math_hard, bfcl_chat, ifeval__strict, math, mbpp_plus.

**Base PreTrained Model**

![[Pasted image 20241212181624.png|600]]

# PreTrain BaseModel
## Llama 3.1

[Llama Eval Detail](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md)

| Dataset Name    | few-shot | cot? | max_out_len | remarks           | 现状数据集情况 |
| --------------- | -------- | ---- | ----------- | ----------------- | ------- |
| agieval_english | 0        | ❌    | 10          | default settings  |         |
| arc_challenge   | 25       | -    | -           | ppl               |         |
| bbh             | -        | -    | -           |                   |         |
| boolq           | 0        | -    | -           | ppl               |         |
| commonseqa      | 8        | ✅    | -           | ppl 我们的不包含cot     |         |
| drop            | 3        | ✅    | 32k         | gen               |         |
| mmlu            | 5        | -    | -           | ppl               |         |
| mmlu_pro        | 5        | ✅    | 512         | gen               |         |
| quac            | 1        | -    | 32          | gen - f1          |         |
| squad           | 1        | ❌    | 32          | gen - exact match |         |
| triviaqa_wiki   | 5        | -    | 24          | gen - exact match |         |
| winogrande      | 5        | -    | -           | ppl               |         |
# PostTrain ChatModel

## Llama3.3

详见：[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md)

| Dataset Name    | few-shot | cot? | max_out_len | remarks           | 现状数据集情况 |
| --------------- | -------- | ---- | ----------- | ----------------- | ------- |
| agieval_english | 0        | ❌    | 10          | default settings  |         |
| arc_challenge   | 0        | -    | 100         | gen               |         |
| bbh             | -        | -    | -           | -                 |         |
| boolq           | -        | -    | -           | gen               |         |
| commonseqa      | 7        | -    | -           | gen               |         |
| drop            | 3        | ✅    | 32k         | gen               |         |
| mmlu            | 5        | ✅    | 1024        | gen               |         |
| mmlu_pro        | 5        | ✅    | 1024        | gen               |         |
| quac            | -        | -    | -           | gen - f1          |         |
| squad           | 1        | ❌    | 32          | gen - exact match |         |
| triviaqa_wiki   | 5        | -    | 24          | gen - exact match |         |
| winogrande      | 5        | -    | -           |                   |         |
### IFEval ✅

**PostTrain Model**

- Default Setting 和现有平台一致 ✅

### HumanEval ✅

- 无特殊说明

### MBPP EvalPlus ❓

**PostTrain Model**

- 0-shot — 现有平台是有shot的，可以跑下试试，数据集名称是 mbpp_plus_gen

### Math

😫 Hard

**PreTrain Model**

- 4-shot max_out_len=512

**PostTrain Model**

- 0-shot cot max_out_len=5120 post process

### GPQA Diamond(subset)

⚠️ 添加配置

**PostTrain Model**

- 0-shot cot exact match max_out_len=2048

### BFCL v2

😆 New

### NIH/Multi-needle

😭

0-shot context lengths 2000 ~ 131072 in 10 intervals max_out_len=256

### MGSM

- 

## 针对BaseModel

数据集有：MMLU、AGIEval English、ARC-Challenge、SQuAD、QuAC、DROP、Needle in Haytack

![[Pasted image 20241212181624.png|600]]

貌似只有Needle in Haystack没有，但这个分比较奇怪，可以暂不考虑，1B的96分，3B和8B的只有1分？

- [x] mmlu — mmlu_ppl
- [x] AGIEval — hh_agieval_english_gen
    - [x] 自定义5-shot randomly
- [x] ARC-Challenge — ARC_c_few_shot_ppl
- [x] SQuAD
    - [x] 自定义1-shot randomly
- [ ] QuAC
- [x] DROP 5-shot cot