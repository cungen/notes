#Area/AI/Eval 
## ‰ªãÁªç

llamaÂÆòÁΩëÊ¶úÂçïÔºö[https://www.llama.com/](https://www.llama.com/)

![[Pasted image 20241212181556.png|600]]

Model CardÈáå‰πüÊúâÊàêÁª©Áõ∏ÂÖ≥ÁöÑÊèèËø∞
[https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)

‰ΩÜÁî±‰∫éÂêÑLlamaÁâàÊú¨3.1~3.3‰ΩøÁî®ÁöÑËØÑÊµãÈõÜ‰∏çÂêåÔºåPreTrainÂíåPostTrainÊâÄ‰ΩøÁî®ÁöÑ‰πü‰∏ç‰∏ÄÊ†∑ÔºåÊâÄ‰ª•Êàë‰ª¨==Âè™ÂØπÈΩêLlama3.1ÁöÑPreTrainÂèäLlama3.3ÁöÑPostTrainÊï∞ÊçÆÈõÜ==
# PreTrain BaseModel
## Llama 3.1

ËØÑÊµãÊï∞ÊçÆÔºö[Llama-3.1-70B-evals](https://huggingface.co/datasets/meta-llama/Llama-3.1-70B-evals/tree/main/Llama-3.1-70B-evals)
Êï∞ÊçÆÈõÜËØ¥ÊòéÔºö[Llama Eval Detail](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md)
[3.1 ModelCard](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)

> "-"Ë°®Á§∫Êú™ÊòéÁ°ÆËØ¥Êòé
> ‰∏Ä‰∫õÊï∞ÊçÆÈõÜÂÆö‰πâÂú®eval detail‰∏≠Âíåmodel card‰∏≠Â¶ÇÊûú‰∏ç‰∏ÄËá¥Ôºå‰ª•ModelCard‰∏≠ÁöÑË°®Ê†º‰∏∫ÂáÜ

| type                  | Dataset Name    | few-shot | cot? | max_out_len | remarks           | Áé∞Áä∂Êï∞ÊçÆÈõÜÊÉÖÂÜµÂü∫‰∫éOpenCompass                             |
| --------------------- | --------------- | -------- | ---- | ----------- | ----------------- | ------------------------------------------------ |
| General               | agieval_english | 3-5      | ‚ùå    | 10          | default settings  | ‚úÖ hh_agieval_english_genÔºåÊ∑ªÂä†few-shot              |
| General               | arc_challenge   | 25       | -    | -           | ppl               | ‚úÖ ARC_c_few_shot_pplÔºå‰ΩÜÂè™Êúâ5-shot                   |
| General               | bbh             | 3        | -    | -           |                   | ‚úÖ bbh_gen                                        |
| Reading comprehension | boolq           | 0        | -    | -           | ppl               | ‚úÖ superglue_boolq_few_shot_ppl Âä†‰∫Üfew-shotÔºåÊïàÊûúÊõ¥Â•Ω‰∏Ä‰∫õ |
| General               | commonseqa      | 8        | ‚úÖ    | -           | ppl Êàë‰ª¨ÁöÑ‰∏çÂåÖÂê´cot     | ‚úÖ commonsenseqa_ppl                              |
| Reading comprehension | drop            | 3        | ‚úÖ    | 32k         | gen               | ‚úÖ drop_gen ‰∏ä‰∏ãÊñáÈïøÂ∫¶‰ΩøÁî®ÈªòËÆ§ÔºåÊ≤°Êúâ32k                       |
| General               | mmlu            | 5        | -    | -           | ppl               | ‚úÖ mmlu_ppl                                       |
| General               | mmlu_pro        | 5        | ‚úÖ    | 512         | gen               | ‚úÖ hh_mmlu_pro_gen Âä†cotÂíåmax_out_len               |
| Reading comprehension | QuAC            | 1        | -    | 32          | gen - f1          | ‚úÖ hh_quac_gen ‰ªÖÂ∞ùËØïÂØπÈΩêÂÖ∂‰∏≠ÁöÑspanÁª¥Â∫¶                     |
| Reading comprehension | SQuAD           | 1        | ‚ùå    | 32          | gen - exact match | ‚úÖ hh_squad_mini_gen Ê∑ªÂä†few-shot                   |
| Knowledge reasoning   | triviaqa_wiki   | 5        | -    | 24          | gen - exact match | ‚úÖ ÈúÄÂèñÂ≠êÈõÜ hh_triviaqa_wiki_gen                      |
| General               | winogrande      | 5        | -    | -           | ppl               | ‚úÖ hh_winogrande_5shot_mini_ppl                   |
### ÂæóÂàÜÂØπÊØî

> **Êï∞ÊçÆÈõÜ**Ë°®Á§∫ÊúâÈóÆÈ¢òÊï∞ÊçÆÈõÜ

| Êï∞ÊçÆÈõÜ\Êù•Ê∫ê              | ModelCard-Llama-3.1 PreTrain | ModelCard-Llama-3.1 PostTrain | Local-Llama-3.1 - Instruct   |
| ------------------- | ---------------------------- | ----------------------------- | ---------------------------- |
| **agieval_english** | **47.8**                     | **-**                         | **20 - hf, 47.22 - vllm**    |
| **arc_challenge**   | **79.7**                     | **83.4**                      | **58.33 - hf, 57.22 - vllm** |
| bbh                 | 61.1                         | -                             | 68.52                        |
| boolq               | 75.7                         | -                             | 75                           |
| commonsenseqa       | 75.0                         | -                             | 74.44                        |
| **drop**            | **58.4**                     | **-**                         | **21.82 - hf, 75.56 - vllm** |
| mmlu                | 66.7                         | 69.4                          | 70.70                        |
| mmlu_pro            | 37.1                         | 48.3                          | 42.31                        |
| **QuAC**            | **44.4**                     | **-**                         | **0 - hf, 37.72 - vllm**     |
| SQuAD               | 76.4                         | -                             | 82.78                        |
| triviaqa_wiki       | 78.5                         | -                             | 78.89                        |
| winogrande          | 60.5                         | -                             | 69.44                        |
# PostTrain ChatModel

## Llama3.3

ËØÑÊµãÊï∞ÊçÆÔºö[Llama-3.3-70B-Instruct-evals](https://huggingface.co/datasets/meta-llama/Llama-3.3-70B-Instruct-evals/tree/main/Llama-3.3-70B-Instruct-evals)
Êï∞ÊçÆÈõÜËØ¥ÊòéÔºö[eval_details.md](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md)
[3.3 ModelCard](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md)

>"-"Ë°®Á§∫Êú™ÊòéÁ°ÆËØ¥Êòé
### [Llama website benchmark](https://www.llama.com/)

| Category              | Dataset Name                                    | Few-Shot Num | CoT | Max Out Len | Current State                             |
| --------------------- | ----------------------------------------------- | ------------ | --- | ----------- | ----------------------------------------- |
| General               | mmlu_0_shot_cot                                 | 0            | ‚úÖ   | 1024        | ‚úÖ hh_mmlu_0shot_gen max_out_len Âè™Êúâ 256    |
| General               | mmlu_pro                                        | 5            | ‚úÖ   | 1024        | ‚úÖ hh_mmlu_pro_gen Âä†cotÂíåmax_out_len        |
| Instruction Following | IFEval                                          | -            | -   | -           | ‚úÖ IFEval_gen                              |
| Code                  | human_eval                                      | 0            | -   | 1024        | ‚úÖ humaneval_gen                           |
| Code                  | mbpp_plus                                       | -            | -   | -           | ‚úÖ mbpp_gen                                |
| Math                  | MATH                                            | 0            | ‚úÖ   | 5120        | ‚úÖ hh_llama_math_gen max_out_len 2048      |
| Math                  | MATH_hard                                       | 0            | ‚úÖ   | 5120        | ‚úÖ hh_llama_math_hard_gen max_out_len 2048 |
| Reasoning             | GPQA Diamond                                    | 0            | ‚úÖ   | 2048        | ‚úÖ gpqa_gen                                |
| Tool use              | Berkeley Function Calling Leaderboard (BFCL) v2 | -            | -   | -           | ‚úÖ hh_bfcl_v2_gen                          |
| Long context          | NIH/Multi-needle                                | 0            | ‚ùå   | 256         | ‚úÖ hh_nih_gen                              |
| Multilingual          | MGSM                                            | 0            | ‚úÖ   | 2048        | ‚úÖ mgsm_gen                                |

#### ÂæóÂàÜÂØπÊØî

| Êï∞ÊçÆÈõÜ\Êù•Ê∫ê                                          | [ModelCard-Llama-3.1](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md#instruction-tuned-models) | Local-Llama-3.1-Instruct                                        |
| ----------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- |
| mmlu_0_shot_cot                                 | **73**                                                                                                                             | **39.65** ÔºàocÂéüÁîüÔºåÈÄâÈ°πÊó†ÔºåÊúâÈóÆÈ¢òÔºâ<br>72.13 - Ëá™ÂÆö‰πâconfig                   |
| mmlu_pro                                        | 48.3                                                                                                                               | 47.25                                                           |
| IFEval                                          | 80.4                                                                                                                               | 67.10                                                           |
| **human_eval**                                  | **72.6**                                                                                                                           | **23.17** - humaneval_gen<br>**20.73** - humaneval_plus_gen     |
| mbpp_plus                                       | 72.8                                                                                                                               | 53.89                                                           |
| MATH                                            | -                                                                                                                                  | 21.67                                                           |
| **MATH_hard**                                   | **25.4**                                                                                                                           | **5.0**                                                         |
| GPQA Diamond                                    | 31.8                                                                                                                               | 26.67                                                           |
| Berkeley Function Calling Leaderboard (BFCL) v2 | 65.4                                                                                                                               | 63.89                                                           |
| NIH/Multi-needle                                | 98.8                                                                                                                               | 51.3<br>- ‰∏ä‰∏ãÊñá‰∏∫64kÊó∂Ôºå3.1-8BÊúÄÂ§ß‰∏∫100kÂ∑¶Âè≥                              |
| **MGSM**                                        | **68.9**                                                                                                                           | **27.27** - max_out_len‰∏∫512Â§™Áü≠ÔºåÊîπ‰∏∫2048ËØï‰∏ã<br>**25.57** - 2048ÔºåÁî®Â§Ñ‰∏çÂ§ß |

### [Eval Detail Datasets](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/eval_details.md)

| Dataset Name      | Few-Shot Num | CoT | Max Out Len | Current State                  |
| ----------------- | ------------ | --- | ----------- | ------------------------------ |
| TLDR9+            | 1            | ‚ùå   | 512         |                                |
| Open-Rewrite      | 0            | ‚ùå   | 512         |                                |
| ARC-Challenge     | 25           | ‚ùå   | 100         | ‚úÖ ARC_c_few_shot_pplÔºå‰ΩÜÂè™Êúâ5-shot |
| AGIEval English   | -            | -   | 10          | ‚úÖ hh_agieval_english_gen       |
| SQuAD             | 1            | ‚ùå   | 32          | üõ† ÈúÄÊ∑ªÂä†few-shot                 |
| QuAC              | 1            | ‚ùå   | 32          |                                |
| DROP              | 3            | ‚ùå   | 32          | ‚úÖ drop_gen                     |
| GSM8K             | 8            | ‚úÖ   | 1024        | ‚úÖ gsm8k_gen max_out_len 512    |
| InfiniteBench     | 0            | ‚ùå   | 20          |                                |
| Multilingual MMLU | 5            | ‚ùå   | 10          |                                |
| Nexus             | -            | -   | -           |                                |
| RULER             | -            | -   | -           | ‚úÖ                              |
| MMMU              | 0            | ‚úÖ   | 2048        |                                |
| MMMU-Pro standard | 0            | ‚ùå   | 2048        |                                |
| MMMU-Pro vision   | 0            | ‚ùå   | 2048        |                                |
| AI2D              | 0            | ‚ùå   | 400         |                                |
| ChartQA           | 0            | ‚úÖ   | 512         |                                |
| DocVQA            | 0            | ‚ùå   | 512         |                                |
| VQAv2             | 0            | ‚ùå   | 25          |                                |
| MathVista         | 0            | ‚ùå   | 2048        |                                |
